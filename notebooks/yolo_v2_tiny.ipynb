{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xv0urKZ_l08V",
    "outputId": "688b00d8-cca0-4634-eb04-f360790cc2a0"
   },
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as FT\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# from IPython import display\n",
    "# display.clear_output()\n",
    "\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive',force_remount=True)\n",
    "\n",
    "dataset_path = \"LOCAL\" #LOCAL/DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NR7YA7cnSK1w"
   },
   "outputs": [],
   "source": [
    "def show_bbox(img,boxes,axis,color=(0,255,0)):\n",
    "    img=img.copy()\n",
    "    for i,box in enumerate(boxes):\n",
    "        box = [ round(elem) for elem in box ]\n",
    "        cv2.rectangle(img,(box[0],box[1]),(box[2],box[3]),color,2)\n",
    "        y=box[1]-10 if box[1]-10>10 else box[1]+10\n",
    "\n",
    "    axis.imshow(img)\n",
    "    axis.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BzWtok1GkA6W"
   },
   "outputs": [],
   "source": [
    "def extract_box(img_dict):\n",
    "  boxes=[]\n",
    "  for key in img_dict['image']:\n",
    "    boxes.append([img_dict['xmin'][key],img_dict['ymin'][key],img_dict['xmax'][key],img_dict['ymax'][key]])\n",
    "  return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQTq5aT_MCEp"
   },
   "outputs": [],
   "source": [
    "#JUST TO SHOW BOXES EXAMPLES\n",
    "\n",
    "# img_number=8\n",
    "# img_column=2\n",
    "# fig,axes=plt.subplots(int(math.ceil(img_number/img_column)),img_column,figsize=(12,12))\n",
    "# plt.subplots_adjust(wspace=0.1,hspace=0.1)\n",
    "# ax=axes.flatten()\n",
    "\n",
    "\n",
    "# # Drive Path\n",
    "# drive_dir=os.path.join(os.getcwd(),\"drive\",\"MyDrive\",\"archive\",\"data\")\n",
    "\n",
    "# #Dataframe Creation\n",
    "# csv_path=os.path.join(drive_dir,\"train_solution_bounding_boxes (1).csv\")\n",
    "# df = pd.read_csv(csv_path)\n",
    "\n",
    "# # Images From Directory\n",
    "# test_imgs_dir=os.path.join(drive_dir,\"training_images\")\n",
    "# test_img_list = os.listdir(test_imgs_dir)\n",
    "# imgs_name=np.random.choice(test_img_list,img_number)\n",
    "# print(imgs_name)\n",
    "# print('\\n\\n')\n",
    "# print('Loop:')\n",
    "# for i,img_name in enumerate(imgs_name):\n",
    "#     img_file_path=os.path.join(test_imgs_dir,img_name)\n",
    "#     print(str(i)+': '+img_name)\n",
    "#     img_dict=df.loc[df['image'] == img_name].to_dict()\n",
    "#     print('boxes: '+str(extract_box(img_dict)))\n",
    "#     img=cv2.imread(img_file_path)\n",
    "#     img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "#     boxes=[extract_box(img_dict)]\n",
    "#     show_bbox(img,boxes[0],axis=ax[i])\n",
    "\n",
    "# plt.savefig(\"1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1r3WGhvIn2bH",
    "outputId": "c29cf6f1-1b48-412f-be16-a1ed195bbcfa"
   },
   "outputs": [],
   "source": [
    "#NORMALIZE ALL DATA RELATED MTO THE \"BIGGEST\" ONE, NOT TO THE IMAGES SIZES\n",
    "\n",
    "# Drive Path\n",
    "if dataset_path == \"LOCAL\":\n",
    "    drive_dir=os.path.join(os.getcwd(), \"archive\",\"data\")\n",
    "elif dataset_path == \"DRIVE\":\n",
    "    drive_dir=os.path.join(os.getcwd(),\"drive\",\"MyDrive\",\"archive\",\"data\")\n",
    "\n",
    "\n",
    "#Dataframe Creation\n",
    "csv_path=os.path.join(drive_dir,\"train_solution_bounding_boxes (1).csv\")\n",
    "\n",
    "df=pd.read_csv(csv_path)\n",
    "\n",
    "width=676\n",
    "height=380\n",
    "\n",
    "df[\"class\"]=0\n",
    "df.rename(columns={'image':'img_name'}, inplace=True)\n",
    "\n",
    "df[\"x_centre\"]=(df[\"xmin\"]+df[\"xmax\"])/2\n",
    "df[\"y_centre\"]=(df[\"ymin\"]+df[\"ymax\"])/2\n",
    "df[\"width\"]=(df[\"xmax\"]-df[\"xmin\"])\n",
    "df[\"height\"]=(df[\"ymax\"]-df[\"ymin\"])\n",
    "\n",
    "#normalizing bounding box coordinates\n",
    "df[\"x_centre\"]=df[\"x_centre\"]/width\n",
    "df[\"y_centre\"]=df[\"y_centre\"]/height\n",
    "df[\"width\"]=df[\"width\"]/width\n",
    "df[\"height\"]=df[\"height\"]/height\n",
    "\n",
    "df_yolo=df[[\"img_name\",\"class\",\"x_centre\",\"y_centre\",\"width\",\"height\"]]\n",
    "df_yolo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tflQ49luo1jy"
   },
   "outputs": [],
   "source": [
    "# #RUNNARE SOLO UNA VOLTA\n",
    "\n",
    "train_imgs_dir=os.path.join(drive_dir,\"training_images\")\n",
    "labels_dir=os.path.join(drive_dir,\"label\")\n",
    "images_dir=os.path.join(drive_dir,\"image\")\n",
    "imgs_list=list(sorted(os.listdir(train_imgs_dir)))\n",
    "\n",
    "# incompatible_imgs = []\n",
    "\n",
    "# for idx, img_name in tqdm(enumerate(imgs_list)):\n",
    "\n",
    "#     if np.isin(img_name, df_yolo[\"img_name\"]):\n",
    "#         columns=[\"class\",\"x_centre\",\"y_centre\",\"width\",\"height\"]\n",
    "#         img_bbox=df_yolo[df_yolo[\"img_name\"]==img_name][columns].values\n",
    "\n",
    "#         label_file_path=os.path.join(labels_dir,img_name[:-4]+\".txt\")\n",
    "#         with open(label_file_path,\"w+\") as f:\n",
    "#             for row in img_bbox:\n",
    "#                 text=\" \".join(row.astype(str))\n",
    "#                 f.write(text)\n",
    "#                 f.write(\"\\n\")\n",
    "\n",
    "#         old_image_path=os.path.join(train_imgs_dir,img_name)\n",
    "#         new_image_path=os.path.join(images_dir,img_name)\n",
    "\n",
    "#         #copy images from training_images to image directory\n",
    "#         shutil.copy(old_image_path,new_image_path)\n",
    "#     else:\n",
    "#       incompatible_imgs.append(img_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zX7NQ2OxpsG"
   },
   "outputs": [],
   "source": [
    "#STUFF TO CHECK DATA\n",
    "\n",
    "# list(df_yolo[\"img_name\"])\n",
    "\n",
    "# # shutil.rmtree(labels_dir)\n",
    "# # shutil.rmtree(images_dir)\n",
    "\n",
    "# # os.mkdir(labels_dir)\n",
    "# # os.mkdir(images_dir)\n",
    "\n",
    "# print(len(list(os.listdir(labels_dir))))\n",
    "# print(len(list(os.listdir(images_dir))))\n",
    "# print(len(list(os.listdir(train_imgs_dir))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xPhyYhOps5sO"
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, S, S, 4) #4 because only 1 box is passed per time\n",
    "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, S, S, 4)\n",
    "        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    # print(\"boxes_preds: \", boxes_preds.shape)\n",
    "    # print(\"boxes_labels: \", boxes_labels.shape)\n",
    "\n",
    "    if box_format == \"midpoint\" :\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2  # x - (w / 2)\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2  # y - (h / 2)\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2  # x + (w / 2)\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2  # y + (h / 2)\n",
    "\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    if box_format == \"corners\" :\n",
    "        box1_x1, box1_y1 = boxes_preds[..., 0:1], boxes_preds[..., 1:2]\n",
    "        box1_x2, box1_y2 = boxes_preds[..., 2:3], boxes_preds[..., 3:4]\n",
    "        box2_x1, box2_y1 = boxes_labels[..., 0:1], boxes_labels[..., 1:2]\n",
    "        box2_x2, box2_y2 = boxes_labels[..., 2:3], boxes_labels[..., 3:4]\n",
    "\n",
    "    x1, y1 = torch.max(box1_x1, box2_x1), torch.max(box1_y1, box2_y1) #greater upper-left corner\n",
    "    x2, y2 = torch.min(box1_x2, box2_x2), torch.min(box1_y2, box2_y2) #lower bottom-right corner\n",
    "\n",
    "\n",
    "    '''\n",
    "    limits the range to a lower bound of 0, so to avoid negative values.\n",
    "    inter computes the area, how?\n",
    "    '''\n",
    "\n",
    "    inter = (x2-x1).clamp(0) * (y2-y1).clamp(0)\n",
    "\n",
    "    box1_area = abs( (box1_x2 - box1_x1) * (box1_y2 - box1_y1) ) #base * altezza\n",
    "    box2_area = abs( (box2_x2 - box2_x1) * (box2_y2 - box2_y1) )\n",
    "\n",
    "    uni = box1_area + box2_area - inter\n",
    "\n",
    "    return inter / (uni + 1e-6)\n",
    "\n",
    "\n",
    "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU)\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "                box_format=box_format,\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms\n",
    "\n",
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision\n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        num_classes (int): number of classes\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    avg_precisions = []\n",
    "    epsilon = 1e-6 # used for numerical stability\n",
    "    for c in range(num_classes):\n",
    "        detections, ground_truths = [], []\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c: detections.append(detection)\n",
    "        for true_box in true_boxes :\n",
    "            if true_box[1] == c: ground_truths.append(true_box)\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "        for key,val in amount_bboxes.items(): amount_bboxes[key] = torch.zeros(val)\n",
    "        detections.sort(key=lambda x:x[2], reverse=True)\n",
    "        TP, FP, total_true_bboxes = torch.zeros(len(detections)), torch.zeros(len(detections)), len(ground_truths)\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            ground_truth_img = [ bbox for bbox in ground_truths if bbox[0] == detection[0] ]\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "            for idx, gt in enumerate(ground_truth_img) :\n",
    "                iou = intersection_over_union(torch.tensor(detection[3:]), torch.tensor(gt[3:]), box_format=box_format,)\n",
    "                if iou > best_iou :\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "            if best_iou > iou_threshold :\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else :\n",
    "                    FP[detection_idx] = 1\n",
    "            else :\n",
    "                FP[detection_idx] = 1\n",
    "        TP_cumsum, FP_cumsum = torch.cumsum(TP, dim=0), torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions, recalls = torch.cat((torch.tensor([1]), precisions)), torch.cat((torch.tensor([0]), recalls))\n",
    "        avg_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(avg_precisions) / (len(avg_precisions) + epsilon)\n",
    "\n",
    "\n",
    "def plot_image(image, boxes):\n",
    "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
    "    im = np.array(image)\n",
    "    height, width, _ = im.shape\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(im)\n",
    "    for box in boxes:\n",
    "        box = box[2:]\n",
    "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width, box[3] * height,\n",
    "            linewidth=1, edgecolor=\"r\", facecolor=\"none\",\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "def get_bboxes(loader, model, iou_threshold, threshold,S=7, B=2, C=1, pred_format=\"cells\", box_format=\"midpoint\", device=\"cuda\"):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels,S,B,C)\n",
    "        bboxes = cellboxes_to_boxes(predictions,S,B,C)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "                box_format=box_format,\n",
    "            )\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n",
    "\n",
    "\n",
    "\n",
    "def convert_cellboxes(predictions, S=7,B=2, C=20):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios.\n",
    "    \"\"\"\n",
    "    num_classes=C\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, 7, 7, B*5+C)\n",
    "    bboxes1 = predictions[..., num_classes+1:num_classes+5]\n",
    "    bboxes2 = predictions[..., num_classes+6:num_classes+10]\n",
    "    scores = torch.cat(\n",
    "        (predictions[..., num_classes].unsqueeze(0), predictions[..., num_classes+5].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
    "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
    "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
    "    predicted_class = predictions[..., :num_classes].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., num_classes], predictions[..., num_classes+5]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
    "    )\n",
    "\n",
    "    return converted_preds\n",
    "\n",
    "\n",
    "def cellboxes_to_boxes(out, S=7,B=2, C=20):\n",
    "    converted_pred = convert_cellboxes(out,S,B,C).reshape(out.shape[0], S * S, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "\n",
    "        for bbox_idx in range(S * S):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "\n",
    "    return all_bboxes\n",
    "\n",
    "def makeCSVfile(dataset, folder):\n",
    "    images = subprocess.check_output(\"ls \"+ dataset.location + folder + '/images', shell=True, text=True).split('\\n')[:-1]\n",
    "    labels = subprocess.check_output(\"ls \"+ dataset.location + folder + '/labels', shell=True, text=True).split('\\n')[:-1]\n",
    "    return pd.DataFrame(data = {'image': images, 'text':labels})\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\", exit_training=False):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "    if exit_training : exit()\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "hNlYwv_nLHXw"
   },
   "outputs": [],
   "source": [
    "# Conv2d BatchNorm2d LeakyReLU MaxPool2d\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "architecture_config = [\n",
    "    (7, 64, 2, 3),\n",
    "    \"M\",\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1),\n",
    "    (1, 256, 1, 0),\n",
    "    (3, 512, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0),\n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "]\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels) # not present in the original YOLO model\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x): return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "\n",
    "class Yolov1(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super().__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        # print(f'output shape: {x.shape}')\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "\n",
    "    def _create_conv_layers(self, architecture):\n",
    "        in_channels = self.in_channels\n",
    "        layers = []\n",
    "        for x in architecture:\n",
    "            if type(x)==tuple:\n",
    "                layers += [ CNNBlock(in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3]) ]\n",
    "                in_channels = x[1]\n",
    "            elif type(x)==str:\n",
    "                layers += [ nn.MaxPool2d(kernel_size=2, stride=2) ]\n",
    "            elif type(x)==list:\n",
    "                conv1, conv2, num_repeats = x[0], x[1], x[2]\n",
    "                for i in range(num_repeats):\n",
    "                    layers += [ CNNBlock(in_channels, conv1[1], kernel_size=conv1[0], stride=conv1[2], padding=conv1[3]) ]\n",
    "                    layers += [ CNNBlock(conv1[1], conv2[1], kernel_size=conv2[0], stride=conv2[2], padding=conv2[3]) ]\n",
    "                    in_channels = conv2[1]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024*S*S,496), #4096 in YOLO paper\n",
    "            nn.Dropout(0.0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(496, S*S*(B*5+C))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "zY5qG04g9Baw"
   },
   "outputs": [],
   "source": [
    "architecture_config = [\n",
    "    (3, 16, 1, 1),\n",
    "    \"M-2\",\n",
    "    (3, 32, 1, 1),\n",
    "    \"M-2\",\n",
    "    (3, 64, 1, 1),\n",
    "    \"M-2\",\n",
    "    (3, 128, 1, 1),\n",
    "    \"M-2\",\n",
    "    (3, 256, 1, 1),\n",
    "    \"M-2\",\n",
    "    (3, 512, 1, 1),\n",
    "    \"M-2\",\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    (1, 1024, 1, 0)\n",
    "]\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels) # not present in the original YOLO model\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x): return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "\n",
    "class Yolov2Tiny(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super().__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        # print(f'output shape: {x.shape}')\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "\n",
    "    def _create_conv_layers(self, architecture):\n",
    "        in_channels = self.in_channels\n",
    "        layers = []\n",
    "        for x in architecture:\n",
    "            if type(x)==tuple:\n",
    "                layers += [ CNNBlock(in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3]) ]\n",
    "                in_channels = x[1]\n",
    "            elif type(x)==str:\n",
    "                stride = int(x.split(\"-\")[1])\n",
    "                layers += [ nn.MaxPool2d(kernel_size=2, stride=stride) ]\n",
    "            elif type(x)==list:\n",
    "                conv1, conv2, num_repeats = x[0], x[1], x[2]\n",
    "                for i in range(num_repeats):\n",
    "                    layers += [ CNNBlock(in_channels, conv1[1], kernel_size=conv1[0], stride=conv1[2], padding=conv1[3]) ]\n",
    "                    layers += [ CNNBlock(conv1[1], conv2[1], kernel_size=conv2[0], stride=conv2[2], padding=conv2[3]) ]\n",
    "                    in_channels = conv2[1]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024*S*S,496), #4096 in YOLO paper\n",
    "            nn.Dropout(0.0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(496, S*S*(B*5+C))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "o2tB9QgoWYpf"
   },
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "        self.S, self.B, self.C = S, B, C\n",
    "        self.lambda_coord, self.lambda_noobj = 5, .5\n",
    "\n",
    "    def forward(self, predictions, target ):\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.B*5+self.C)\n",
    "        ious = torch.cat(\n",
    "            (intersection_over_union(predictions[...,self.C+1:self.C+5], target[...,self.C+1:self.C+5]).unsqueeze(0),  #unsqueeze(0) works only if b_size == 1\n",
    "            intersection_over_union(predictions[...,self.C+6:self.C+10], target[...,self.C+1:self.C+5]).unsqueeze(0)) # target = [1, 1, x, y, w, h, 0,0,0,0,0]\n",
    "                                                                                                                      #pred = [0.8, 0, 0.6, 0.8, 0.5, 0.7, 1, 0.5, 0.4,\n",
    "        )\n",
    "\n",
    "        '''\n",
    "            CASE 1 CLASS:\n",
    "            C+1:C+5 -> 2:6 (4) ELEMENTS\n",
    "            C+6:C+10 -> 7:11 (4) ELEMENTS\n",
    "            element in position [C+5] is excluded\n",
    "        '''\n",
    "\n",
    "        _, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[...,self.C:self.C+1]\n",
    "\n",
    "        box_predictions =  exists_box * ( (1-bestbox)*predictions[...,self.C+1:self.C+5] + bestbox*predictions[...,self.C+6:self.C+10] )\n",
    "        box_predictions[...,2:4] = torch.sign(box_predictions[...,2:4])*torch.sqrt(torch.abs(box_predictions[...,2:4]+1e-6)) # WOW\n",
    "        box_targets = exists_box * target[...,self.C+1:self.C+5] #Just these 4 because it can predicts at most 1 box for each cell\n",
    "        box_targets[...,2:4] = torch.sqrt(box_targets[...,2:4]) # rad(w), rad(h)\n",
    "\n",
    "        # print(\"box_predictions: \", box_targets.shape)\n",
    "        # print(\"box_targets: \", box_targets.shape)\n",
    "        # print(\"box_predictions_flatten: \", torch.flatten(box_predictions, end_dim=-2).shape)\n",
    "        # print(\"box_targets_flatten: \", torch.flatten(box_targets, end_dim=-2).shape)\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2), # batch_size*S*S, 4 e.g. (16*7*7=784, 4)\n",
    "            torch.flatten(box_targets, end_dim=-2)      # batch_size*S*S, 4\n",
    "        )\n",
    "        '''\n",
    "        object_loss is greater if my model predicted the correct box between the B possible boxes\n",
    "        C:C+1 and C+5:C+6 are 1 if the chosen box is respectively the first or the second\n",
    "        '''\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * ( (1-bestbox)*predictions[...,self.C:self.C+1] + bestbox*predictions[...,self.C+5:self.C+6] )),\n",
    "            torch.flatten(exists_box)\n",
    "        )\n",
    "        '''\n",
    "        no_object_loss exists when no object are in the image\n",
    "        else it penalize the loss when the model predicted a box or worse if the model predicted both\n",
    "        '''\n",
    "        no_object_loss=self.mse(\n",
    "            torch.flatten((1-exists_box) * predictions[...,self.C:self.C+1], end_dim=-2),\n",
    "            torch.flatten((1-exists_box) * target[...,self.C:self.C+1], end_dim=-2)\n",
    "        ) + self.mse(\n",
    "            torch.flatten((1-exists_box) * predictions[...,self.C+5:self.C+6], end_dim=-2),\n",
    "            torch.flatten((1-exists_box) * target[...,self.C:self.C+1], end_dim=-2)\n",
    "        )\n",
    "        '''\n",
    "        penilize if the model predicted the wrong class\n",
    "        NOTE: this can be remove if the class is just one\n",
    "        '''\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[...,:self.C], end_dim=-2),\n",
    "            torch.flatten(exists_box * target[...,:self.C], end_dim=-2)\n",
    "        )\n",
    "        loss = (self.lambda_coord*box_loss + object_loss + self.lambda_noobj*no_object_loss + class_loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "0HUX2QFY9Bax"
   },
   "outputs": [],
   "source": [
    "class YoloLossMultiBoxes(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YoloLossMultiBoxes, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "        self.S, self.B, self.C = S, B, C\n",
    "        self.lambda_coord, self.lambda_noobj = 5, .5\n",
    "\n",
    "    def forward(self, predictions, target ):\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.B*5+self.C)\n",
    "        intersection_tuples = ()\n",
    "\n",
    "        for i_1 in range(self.B):\n",
    "            i_o_u = intersection_over_union(predictions[...,self.C+(1+i_1*5):self.C+(5+i_1*5)], target[...,self.C+1:self.C+5]).unsqueeze(0)\n",
    "            intersection_tuples += (i_o_u,)\n",
    "\n",
    "        # i_o_u = intersection_over_union(target[...,self.C+1:self.C+5], target[...,self.C+1:self.C+5]).unsqueeze(0)\n",
    "        # intersection_tuples += (i_o_u,)\n",
    "\n",
    "        ious = torch.cat(intersection_tuples) # B X batch_size X S X S X self.B*5+self.C\n",
    "\n",
    "        _, bestbox = torch.max(ious, dim=0) # Why on dimension 0 and not -1?\n",
    "        exists_box = target[...,self.C:self.C+1]\n",
    "\n",
    "        # extraction_classes = target[...,0:self.C]\n",
    "        '''\n",
    "        le prime C elems sono relativi a le probabilitÃ  per ogni classe,\n",
    "\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "        # print(f'bestbox: {bestbox}')\n",
    "        # print(f'exists_box shape: {exists_box.shape}, bestbox shape: {bestbox.shape}, target shape: {target.shape}, target POST shape: {target[...,0].shape}, predictions shape: {predictions.shape}')\n",
    "\n",
    "        # predictions = [\n",
    "        #   [\n",
    "        #     [[1, 0, 0.5, 0.7, 0.9, 0.3, 0, 0.3, 0.4, 0.6, 0.4, 0, 0.3, 0.4, 0.6, 0.4]], [[1, 0, 0.5, 0.7, 0.9, 0.3, 0, 0.3, 0.4, 0.6, 0.4, 0, 0.3, 0.4, 0.6, 0.4]]\n",
    "        #   ],\n",
    "        #   [\n",
    "        #     [[1, 0, 0.5, 0.7, 0.9, 0.3, 0, 0.3, 0.4, 0.6, 0.4, 0, 0.3, 0.4, 0.6, 0.4]], [[1, 0, 0.5, 0.7, 0.9, 0.3, 0, 0.3, 0.4, 0.6, 0.4, 0, 0.3, 0.4, 0.6, 0.4]]\n",
    "        #   ]\n",
    "        # ]\n",
    "\n",
    "        # bestbox = [\n",
    "        #   [\n",
    "        #     [0], [1]\n",
    "        #   ],\n",
    "        #   [\n",
    "        #     [2], [1]\n",
    "        #   ]\n",
    "        # ]\n",
    "\n",
    "        # exists_box = [\n",
    "        #   [\n",
    "        #     [0], [1]\n",
    "        #   ],\n",
    "        #   [\n",
    "        #     [0], [0]\n",
    "        #   ]\n",
    "        # ]\n",
    "\n",
    "        # preds_box = (1 - bestbox)\n",
    "        # preds_box[preds_box != 1] = 0\n",
    "        # box_predictions = preds_box * predictions[...,self.C+(1):self.C+(5)]\n",
    "\n",
    "        pred_shape = predictions.shape[0:3]\n",
    "        pred_shape = list(pred_shape)\n",
    "        pred_shape.append(4)\n",
    "\n",
    "        box_predictions = torch.zeros(pred_shape).to(DEVICE) # batch_size x S x S x 4\n",
    "\n",
    "        for i_1 in range(self.B): #prendo solo quello relativo all'argmax\n",
    "            # preds_box = ((1 + i_1) - bestbox)\n",
    "            preds_box = bestbox.detach().clone()\n",
    "            if i_1 == 0:\n",
    "              preds_box[preds_box == i_1] = -1\n",
    "              preds_box[preds_box != -1] = 0\n",
    "              preds_box[preds_box == -1] = 1\n",
    "            else:\n",
    "              preds_box[preds_box != i_1] = 0\n",
    "              preds_box[preds_box == i_1] = 1\n",
    "\n",
    "            box_predictions += preds_box * predictions[...,self.C+(1+i_1*5):self.C+(5+i_1*5)]\n",
    "\n",
    "            # box_predictions += i_1 * predictions[...,self.C+(1+i_1*5):self.C+(5+i_1*5)] # da eliminare\n",
    "\n",
    "        box_predictions = exists_box * box_predictions\n",
    "        box_predictions[...,2:4] = torch.sign(box_predictions[...,2:4])*torch.sqrt(torch.abs(box_predictions[...,2:4]+1e-6)) # WOW\n",
    "        box_targets = exists_box * target[...,self.C+1:self.C+5] #Just these 4 because it can predicts at most 1 box for each cell\n",
    "        box_targets[...,2:4] = torch.sqrt(box_targets[...,2:4]) # rad(w), rad(h)\n",
    "\n",
    "\n",
    "        '''\n",
    "            CASE 1 CLASS:\n",
    "            C+1:C+5 -> 2:6 (4) ELEMENTS\n",
    "            C+6:C+10 -> 7:11 (4) ELEMENTS\n",
    "            element in position [C+5] is excluded\n",
    "        '''\n",
    "\n",
    "#         box_predictions =  exists_box * ( (1-bestbox)*predictions[...,self.C+1:self.C+5] + bestbox*predictions[...,self.C+6:self.C+10] )\n",
    "#         box_predictions[...,2:4] = torch.sign(box_predictions[...,2:4])*torch.sqrt(torch.abs(box_predictions[...,2:4]+1e-6)) # WOW\n",
    "#         box_targets = exists_box * target[...,self.C+1:self.C+5] #Just these 4 because it can predicts at most 1 box for each cell\n",
    "#         box_targets[...,2:4] = torch.sqrt(box_targets[...,2:4]) # rad(w), rad(h)\n",
    "\n",
    "        # print(\"box_predictions: \", box_targets.shape)\n",
    "        # print(\"box_targets: \", box_targets.shape)\n",
    "        # print(\"box_predictions_flatten: \", torch.flatten(box_predictions, end_dim=-2).shape)\n",
    "        # print(\"box_targets_flatten: \", torch.flatten(box_targets, end_dim=-2).shape)\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2), # batch_size*S*S, 4 e.g. (16*7*7=784, 4)\n",
    "            torch.flatten(box_targets, end_dim=-2)      # batch_size*S*S, 4\n",
    "        )\n",
    "        '''\n",
    "        object_loss is greater if my model predicted the correct box between the B possible boxes\n",
    "        C:C+1 and C+5:C+6 are 1 if the chosen box is respectively the first or the second\n",
    "        '''\n",
    "\n",
    "        # print(f'exists_box shape: {exists_box.shape}, bestbox shape: {bestbox.shape}, target shape: {target.shape}, target POST shape: {target[...,0:1].shape}, predictions shape: {predictions.shape} predictions POST shape: {predictions[...,self.C:self.C+1].shape}')\n",
    "        pred_shape = predictions.shape[0:3]\n",
    "        pred_shape = list(pred_shape)\n",
    "        pred_shape.append(1)\n",
    "        preds = torch.zeros(pred_shape).to(DEVICE) # batch_size x S x S x 1\n",
    "\n",
    "        for i_1 in range(self.B): #prendo solo quello relativo all'argmax\n",
    "            # preds_box = ((1 + i_1) - bestbox)\n",
    "            preds_box = bestbox.detach().clone()\n",
    "            if i_1 == 0:\n",
    "              preds_box[preds_box == i_1] = -1\n",
    "              preds_box[preds_box != -1] = 0\n",
    "              preds_box[preds_box == -1] = 1\n",
    "            else:\n",
    "              preds_box[preds_box != i_1] = 0\n",
    "              preds_box[preds_box == i_1] = 1\n",
    "\n",
    "            preds += preds_box * predictions[...,self.C+(i_1*5):self.C+(1+i_1*5)]\n",
    "\n",
    "        preds = exists_box * preds\n",
    "\n",
    "\n",
    "        # preds = target[...,0:1] * predictions[...,self.C:self.C+1]\n",
    "        # for i_1 in range(1, self.B): #prendo solo quello relativo all'argmax\n",
    "        #     preds += target[...,i_1:i_1+1] * predictions[...,self.C+(i_1*5):self.C+(1+i_1*5)]\n",
    "        # preds = exists_box * preds\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(preds),\n",
    "            torch.flatten(exists_box)\n",
    "        )\n",
    "        '''\n",
    "        no_object_loss exists when no object are in the image\n",
    "        else it penalize the loss when the model predicted a box or worse if the model predicted both\n",
    "        '''\n",
    "        no_object_loss = self.mse(\n",
    "                torch.flatten((1-exists_box) * predictions[...,self.C:self.C+1], end_dim=-2),\n",
    "                torch.flatten((1-exists_box) * target[...,self.C:self.C+1], end_dim=-2)\n",
    "            )\n",
    "        for i_1 in range(1, self.B): #prendo solo quello relativo all'argmax\n",
    "            no_object_loss += self.mse(\n",
    "                torch.flatten((1-exists_box) * predictions[...,self.C+(i_1*5):self.C+(1+i_1*5)], end_dim=-2),\n",
    "                torch.flatten((1-exists_box) * target[...,self.C:self.C+1], end_dim=-2)\n",
    "            )\n",
    "        '''\n",
    "        penilize if the model predicted the wrong class\n",
    "        NOTE: this can be remove if the class is just one\n",
    "        '''\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[...,:self.C], end_dim=-2),\n",
    "            torch.flatten(exists_box * target[...,:self.C], end_dim=-2)\n",
    "        )\n",
    "        loss = (self.lambda_coord*box_loss + object_loss + self.lambda_noobj*no_object_loss + class_loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pSd9l2wetHRM"
   },
   "outputs": [],
   "source": [
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, label_dir, img_dir, S=7, B=2, C=20, transform=None):\n",
    "        self.annotations = csv_file  # pd.read_csv(csv_file)\n",
    "        self.label_dir, self.img_dir = label_dir, img_dir\n",
    "        self.transform = transform\n",
    "        self.S, self.B, self.C = S, B, C\n",
    "\n",
    "    def __len__(self) : return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])# iloc[index, 1] return row: index, col: 1 = \".txt\" filename\n",
    "        image = Image.open(os.path.join(self.img_dir, self.annotations.iloc[index, 0]))\n",
    "        boxes = []\n",
    "        with open(label_path) as f:\n",
    "            for label in f.readlines():\n",
    "                class_label, x, y, width, height = [float(x) if float(x)!=int(float(x)) else int(float(x)) for x in label.strip().split()]\n",
    "                boxes.append([class_label, x, y, width, height])\n",
    "\n",
    "        if self.transform:\n",
    "          image, boxes = self.transform(image, torch.tensor(boxes))\n",
    "\n",
    "        label_matrix = torch.zeros(self.S, self.S, self.B*5+self.C)\n",
    "\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box #.tolist()\n",
    "            i, j = int(y*self.S), int(x*self.S)\n",
    "            x_cell, y_cell, w_cell, h_cell= x*self.S-j, y*self.S-i, width*self.S, height*self.S #for x and y remove the integer part\n",
    "            if label_matrix[i, j, self.C] == 0: #if no label has been assigned to this label, change it\n",
    "\n",
    "                '''\n",
    "                 #having only 1 class = '0', we will always have the first 2 elems = 1\n",
    "                 l_m[0] => 1 (relative to the correct class)\n",
    "                 l_m[1] => 1 (always 1, relative to the number of classes, relative to the existence of an object)\n",
    "\n",
    "                 #example with 2 classes, and the first(class 0) is the correct one\n",
    "                 l_m[0] => 1 (relative to the correct class)\n",
    "                 l_m[1] => 0 (relative to the wrong)\n",
    "                 l_m[2] => 1 (always 1, relative to the number of classes, relative to the existence of an object)\n",
    "\n",
    "                 then we have always 8 elements relative to the bounding boxes elements\n",
    "\n",
    "                '''\n",
    "\n",
    "                '''\n",
    "\n",
    "                IMPORTANT: this model can predict at most 1 box for cell, in fact is useless to have (B*5+C) it should be enough (5+C):\n",
    "                1: to tell the model there is an object\n",
    "                4: to specify (x, y, w, h)\n",
    "                C: set to 1 the correct class\n",
    "                '''\n",
    "\n",
    "                label_matrix[i, j, self.C], label_matrix[i, j, int(class_label)] = 1, 1\n",
    "                label_matrix[i, j, self.C+1:self.C+5] = torch.tensor([x_cell, y_cell, w_cell, h_cell])\n",
    "\n",
    "        return image,label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fi92IX410ksc"
   },
   "outputs": [],
   "source": [
    "def create_annotations(test_imgs_dir):\n",
    "  test_img_list = os.listdir(test_imgs_dir)\n",
    "  test_label_list = [s.replace(\".jpg\", \".txt\") for s in test_img_list]\n",
    "\n",
    "  return pd.DataFrame(data = {'image': test_img_list, 'text':test_label_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WBl28Z_WtQZ0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main file for training Yolo model on Pascal VOC dataset\n",
    "\"\"\"\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Hyperparameters etc.\n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "# DEVICE = \"CUDA\"\n",
    "BATCH_SIZE = 16 # 64 in original\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 10\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"./overfit.pth.tar\"\n",
    "NUM_CLASSES=1\n",
    "NUM_BOXES=3\n",
    "SPLIT=7\n",
    "#IMG_DIR_TRAIN = f\"{dataset.location.split('/')[-1]}/train/images\"\n",
    "#LABEL_DIR_TRAIN = f\"{dataset.location.split('/')[-1]}/train/labels\"\n",
    "#IMG_DIR_TEST = f\"{dataset.location.split('/')[-1]}/test/images\"\n",
    "#LABEL_DIR_TEST = f\"{dataset.location.split('/')[-1]}/test/labels\"\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "    print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = Yolov2Tiny(split_size=7, num_boxes=NUM_BOXES, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    optimizer = optim.Adam( model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY )\n",
    "    loss_fn = YoloLossMultiBoxes(S=7, B=NUM_BOXES, C=NUM_CLASSES)\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
    "\n",
    "    annotations=create_annotations(images_dir) # LIST LIKE: 0     vid_4_1000.jpg   vid_4_1000.txt\n",
    "\n",
    "    train_dataset=VOCDataset(annotations, labels_dir, images_dir, S=7, B=NUM_BOXES, C=NUM_CLASSES, transform=transform)\n",
    "\n",
    "\n",
    "    #train_dataset = VOCDataset( makeCSVfile(dataset, '/train'),\n",
    "    #                           transform=transform, img_dir=IMG_DIR_TRAIN, label_dir=LABEL_DIR_TRAIN)\n",
    "\n",
    "    #test_dataset = VOCDataset( makeCSVfile(dataset, '/test'),\n",
    "    #                          transform=transform, img_dir=IMG_DIR_TEST, label_dir=LABEL_DIR_TEST)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE,\n",
    "                              pin_memory=PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "\n",
    "    #test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "    #                         pin_memory=PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        #for x, y in train_loader:\n",
    "        #    x = x.to(DEVICE)\n",
    "        #   for idx in range(8):\n",
    "        #        bboxes = cellboxes_to_boxes(model(x))\n",
    "        #        bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "        #        plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "\n",
    "            # import sys\n",
    "            # sys.exit()\n",
    "\n",
    "        pred_boxes, target_boxes = get_bboxes(train_loader, model, iou_threshold=0.5, threshold=0.4, S=7, B=NUM_BOXES, C=NUM_CLASSES, device=DEVICE)\n",
    "        mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "\n",
    "        print(f\"epoch: {epoch} Train mAP: {mean_avg_prec}\")\n",
    "        k=0\n",
    "        if (mean_avg_prec > 0.9) and (k==0):\n",
    "            k=1\n",
    "            checkpoint = {\n",
    "               \"state_dict\": model.state_dict(),\n",
    "               \"optimizer\": optimizer.state_dict(),\n",
    "            }\n",
    "            save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE, exit_training=True)\n",
    "            # time.sleep(10)\n",
    "\n",
    "        train_fn(train_loader, model, optimizer, loss_fn)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "vrjLSHyMfT5f"
   },
   "outputs": [],
   "source": [
    "NUM_BOXES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "GiUVKi0V2e8u"
   },
   "outputs": [],
   "source": [
    "# model = Yolov1(split_size=7, num_boxes=NUM_BOXES, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY )\n",
    "# loss_fn = YoloLoss(S=7, B=NUM_BOXES, C=NUM_CLASSES)\n",
    "\n",
    "\n",
    "model = Yolov2Tiny(split_size=7, num_boxes=NUM_BOXES, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.Adam( model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY )\n",
    "loss_fn = YoloLossMultiBoxes(S=7, B=NUM_BOXES, C=NUM_CLASSES)\n",
    "\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
    "\n",
    "annotations=create_annotations(images_dir) # LIST LIKE: 0     vid_4_1000.jpg   vid_4_1000.txt\n",
    "\n",
    "dataset=VOCDataset(annotations, labels_dir, images_dir, S=7, B=NUM_BOXES, C=NUM_CLASSES, transform=transform)\n",
    "\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [300, 55])\n",
    "#train_dataset = VOCDataset( makeCSVfile(dataset, '/train'),\n",
    "#                           transform=transform, img_dir=IMG_DIR_TRAIN, label_dir=LABEL_DIR_TRAIN)\n",
    "\n",
    "#test_dataset = VOCDataset( makeCSVfile(dataset, '/test'),\n",
    "#                          transform=transform, img_dir=IMG_DIR_TEST, label_dir=LABEL_DIR_TEST)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE,\n",
    "                          pin_memory=PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE,\n",
    "                        pin_memory=PIN_MEMORY, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wh-hWrv_b6-j",
    "outputId": "2cf682f5-3bdf-4372-e146-3e8405688a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "# checkpoint = {\n",
    "#                \"state_dict\": model.state_dict(),\n",
    "#                \"optimizer\": optimizer.state_dict(),\n",
    "#             }\n",
    "# save_checkpoint(checkpoint, filename=\"drive/MyDrive/Magistrale/Visiope/object_detection/weights_tiny.pth\", exit_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NWBEIOf5nsQ"
   },
   "outputs": [],
   "source": [
    "# #STUFF TO CHECK DATA\n",
    "\n",
    "# image, label_matrix = train_dataset[0]\n",
    "# i, j = int(0.4504 * 7), int(0.5398 * 7)\n",
    "# print(i,j)\n",
    "# k, w = (0.4504 * 7) - int(0.4504 * 7), (0.5398 * 7) - int(0.5398 * 7)\n",
    "# print(k, w)\n",
    "# print(image.shape, label_matrix.shape)\n",
    "# label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTa6DSfH2Z6b",
    "outputId": "a75379ef-32c0-45af-c164-6010dc0bc5f5"
   },
   "outputs": [],
   "source": [
    "map_array = []\n",
    "for epoch in tqdm(range(10)):\n",
    "    # index = 0\n",
    "\n",
    "    # for x, y in train_loader:\n",
    "    #     x = x.to(DEVICE)\n",
    "    #     index += 1\n",
    "    #     for idx in range(8):\n",
    "    #         if index < 2:\n",
    "    #             continue\n",
    "    #         bboxes = cellboxes_to_boxes(model(x), S=7, B=2, C=1)\n",
    "    #         bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "    #         plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "\n",
    "    #     # import sys\n",
    "    #     # sys.exit()\n",
    "\n",
    "    #     if index == 3:\n",
    "    #         break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pred_boxes, target_boxes = get_bboxes(train_loader, model, iou_threshold=0.5, threshold=0.4,S=7, B=NUM_BOXES, C=NUM_CLASSES, device=DEVICE)\n",
    "    mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "    map_array.append(mean_avg_prec.item())\n",
    "    print(f\"epoch: {epoch} Test mAP: {mean_avg_prec}\")\n",
    "    print(f\"mAP Array: {map_array}\")\n",
    "\n",
    "    # print(f\"epoch: {epoch} Train mAP: {mean_avg_prec}\")\n",
    "    # k=0\n",
    "    # if (mean_avg_prec > 0.9) and (k==0):\n",
    "    #     k=1\n",
    "    #     checkpoint = {\n",
    "    #         \"state_dict\": model.state_dict(),\n",
    "    #         \"optimizer\": optimizer.state_dict(),\n",
    "    #     }\n",
    "    #     save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE, exit_training=True)\n",
    "    #     # time.sleep(10)\n",
    "\n",
    "    train_fn(train_loader, model, optimizer, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TqE3uhuY6KZ",
    "outputId": "c9fd5f78-0cec-4837-8947-4ec9faa54a75"
   },
   "outputs": [],
   "source": [
    "map_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "teXQzlqxU6hD",
    "outputId": "e116cac1-705f-4f73-c225-dc45f36c5f85"
   },
   "outputs": [],
   "source": [
    "map_array = []\n",
    "for epoch in tqdm(range(1)):\n",
    "    index = 0\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        index += 1\n",
    "        for idx in range(8):\n",
    "            if index < 2:\n",
    "                continue\n",
    "            bboxes = cellboxes_to_boxes(model(x), S=7, B=NUM_BOXES, C=1)\n",
    "            bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "            plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "\n",
    "        # import sys\n",
    "        # sys.exit()\n",
    "\n",
    "        if index == 3:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pred_boxes, target_boxes = get_bboxes(test_loader, model, iou_threshold=0.5, threshold=0.4,S=7, B=NUM_BOXES, C=NUM_CLASSES, device=DEVICE)\n",
    "    mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "    map_array.append(mean_avg_prec.item())\n",
    "    print(f\"epoch: {epoch} Test mAP: {mean_avg_prec}\")\n",
    "    # print(f\"mAP Array: {map_array}\")\n",
    "\n",
    "    # # print(f\"epoch: {epoch} Train mAP: {mean_avg_prec}\")\n",
    "    # # k=0\n",
    "    # # if (mean_avg_prec > 0.9) and (k==0):\n",
    "    # #     k=1\n",
    "    # #     checkpoint = {\n",
    "    # #         \"state_dict\": model.state_dict(),\n",
    "    # #         \"optimizer\": optimizer.state_dict(),\n",
    "    # #     }\n",
    "    # #     save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE, exit_training=True)\n",
    "    # #     # time.sleep(10)\n",
    "\n",
    "    # train_fn(train_loader, model, optimizer, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyTKEkBt9Bay"
   },
   "outputs": [],
   "source": [
    "# map_array = []\n",
    "# for epoch in tqdm(range(1)):\n",
    "#     #for x, y in train_loader:\n",
    "#     #    x = x.to(DEVICE)\n",
    "#     #   for idx in range(8):\n",
    "#     #        bboxes = cellboxes_to_boxes(model(x))\n",
    "#     #        bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "#     #        plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "\n",
    "#         # import sys\n",
    "#         # sys.exit()\n",
    "\n",
    "#     pred_boxes, target_boxes = get_bboxes(test_loader, model, iou_threshold=0.5, threshold=0.4,S=7, B=NUM_BOXES, C=NUM_CLASSES, device=DEVICE)\n",
    "#     mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "#     map_array.append(mean_avg_prec.item())\n",
    "#     print(f\"epoch: {epoch} Test mAP: {mean_avg_prec}\")\n",
    "#     print(f\"mAP Array: {map_array}\")\n",
    "\n",
    "#     loop = tqdm(train_loader, leave=True)\n",
    "#     mean_loss = []\n",
    "\n",
    "#     S = 7\n",
    "#     B = 3\n",
    "#     C = 1\n",
    "\n",
    "\n",
    "#     for batch_idx, (x, y) in enumerate(loop):\n",
    "#         x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "#         out = model(x)\n",
    "#         target = y\n",
    "#         predictions = out\n",
    "\n",
    "#         predictions = predictions.reshape(-1, S, S, B*5+C)\n",
    "#         intersection_tuples = ()\n",
    "\n",
    "#         for i_1 in range(B - 1):\n",
    "#             i_o_u = intersection_over_union(predictions[...,C+(1+i_1*5):C+(5+i_1*5)], target[...,C+1:C+5]).unsqueeze(0)\n",
    "#             intersection_tuples += (i_o_u,)\n",
    "\n",
    "#         i_o_u = intersection_over_union(target[...,C+1:C+5], target[...,C+1:C+5]).unsqueeze(0)\n",
    "#         intersection_tuples += (i_o_u,)\n",
    "\n",
    "#         ious = torch.cat(intersection_tuples) # B X batch_size X S X S X B*5+C\n",
    "#         print(ious.shape)\n",
    "#         print(ious.shape)\n",
    "\n",
    "#         _, bestbox = torch.max(ious, dim=0) # Why on dimension 0 and not -1?\n",
    "#         exists_box = target[...,C:C+1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         pred_shape = predictions.shape[0:3]\n",
    "#         pred_shape = list(pred_shape)\n",
    "#         pred_shape.append(4)\n",
    "\n",
    "#         box_predictions = torch.zeros(pred_shape).to(DEVICE) # batch_size x S x S x 4\n",
    "#         # print(f'bextbox: {bestbox}')\n",
    "#         for i_1 in range(B): #prendo solo quello relativo all'argmax\n",
    "#             # preds_box = ((1 + i_1) - bestbox)\n",
    "#             preds_box = bestbox.detach().clone()\n",
    "#             if i_1 == 0:\n",
    "#               preds_box[preds_box == i_1] = -1\n",
    "#               preds_box[preds_box != -1] = 0\n",
    "#               preds_box[preds_box == -1] = 1\n",
    "#             else:\n",
    "#               preds_box[preds_box != i_1] = 0\n",
    "#               preds_box[preds_box == i_1] = 1\n",
    "#             # print(f'preds_box: {preds_box}')\n",
    "\n",
    "\n",
    "#             box_predictions += preds_box * predictions[...,C+(1+i_1*5):C+(5+i_1*5)]\n",
    "\n",
    "#             # box_predictions += i_1 * predictions[...,C+(1+i_1*5):C+(5+i_1*5)] # da eliminare\n",
    "\n",
    "#         box_predictions = exists_box * box_predictions\n",
    "#         box_predictions[...,2:4] = torch.sign(box_predictions[...,2:4])*torch.sqrt(torch.abs(box_predictions[...,2:4]+1e-6)) # WOW\n",
    "#         box_targets = exists_box * target[...,C+1:C+5] #Just these 4 because it can predicts at most 1 box for each cell\n",
    "#         box_targets[...,2:4] = torch.sqrt(box_targets[...,2:4]) # rad(w), rad(h)\n",
    "\n",
    "#         break\n",
    "\n",
    "\n",
    "#     #     mean_loss.append(loss.item())\n",
    "#     #     optimizer.zero_grad()\n",
    "#     #     loss.backward()\n",
    "#     #     optimizer.step()\n",
    "\n",
    "#     #     # update progress bar\n",
    "#     #     loop.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "#     # print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")\n",
    "\n",
    "\n",
    "#     # predictions [..., self.C+1:self.C+5]\n",
    "#     # predictions [..., self.C+6:self.C+10]\n",
    "\n",
    "#     # preds = 2 - predictions\n",
    "#     # preds[preds != 1] = 0\n",
    "#     # preds * predictions[...,self.C+11:self.C+15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xY2poLO09Bay",
    "outputId": "0fb4122b-0a00-4808-c635-80bab2b873cd"
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(50)):\n",
    "    #for x, y in train_loader:\n",
    "    #    x = x.to(DEVICE)\n",
    "    #   for idx in range(8):\n",
    "    #        bboxes = cellboxes_to_boxes(model(x))\n",
    "    #        bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "    #        plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "\n",
    "        # import sys\n",
    "        # sys.exit()\n",
    "\n",
    "    # pred_boxes, target_boxes = get_bboxes(train_loader, model, iou_threshold=0.5, threshold=0.4,S=7, B=NUM_BOXES, C=NUM_CLASSES, device=DEVICE)\n",
    "    # mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "\n",
    "    # print(f\"epoch: {epoch} Train mAP: {mean_avg_prec}\")\n",
    "    # k=0\n",
    "    # if (mean_avg_prec > 0.9) and (k==0):\n",
    "    #     k=1\n",
    "    #     checkpoint = {\n",
    "    #         \"state_dict\": model.state_dict(),\n",
    "    #         \"optimizer\": optimizer.state_dict(),\n",
    "    #     }\n",
    "    #     save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE, exit_training=True)\n",
    "    #     # time.sleep(10)\n",
    "\n",
    "    train_fn(train_loader, modelV1, optimizer, loss_fn_v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TinWehj9Bay"
   },
   "outputs": [],
   "source": [
    "modelV1 = Yolov1(split_size=7, num_boxes=NUM_BOXES, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "optimizerV1 = optim.Adam( model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY )\n",
    "loss_fnV1 = YoloLoss(S=7, B=NUM_BOXES, C=NUM_CLASSES)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
