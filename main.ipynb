{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xv0urKZ_l08V",
    "outputId": "688b00d8-cca0-4634-eb04-f360790cc2a0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "from hyperparams import hypers\n",
    "from models.yolo import Yolov1, Yolov2Tiny\n",
    "from models.loss import YoloLossMultiBoxes\n",
    "from train import train_fn\n",
    "from dataprocess.dataset import *\n",
    "from dataprocess.dataprocess import *\n",
    "from dataprocess.dataprocess_coco import *\n",
    "from test import *\n",
    "from utils import *\n",
    "\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import logging\n",
    "# logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s - %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'licenses', 'images', 'annotations', 'categories'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if hypers.DATASET_PATH == \"LOCAL\":\n",
    "    drive_dir=os.path.join(os.getcwd(), \"archive\",\"data\")\n",
    "elif hypers.DATASET_PATH == \"DRIVE\":\n",
    "    drive_dir=os.path.join(os.getcwd(),\"drive\",\"MyDrive\",\"archive\",\"data\")\n",
    "\n",
    "\n",
    "\n",
    "#Dataframe Creation\n",
    "train_labels=os.path.join(drive_dir,\"train_solution_bounding_boxes (1).csv\")\n",
    "train_imgs_dir=os.path.join(drive_dir,\"training_images\")\n",
    "test_imgs_dir=os.path.join(drive_dir,\"testing_images\")\n",
    "\n",
    "labels_dir=os.path.join(drive_dir,\"label\")\n",
    "if os.path.exists(labels_dir) == False:\n",
    "  os.mkdir(labels_dir)\n",
    "\n",
    "images_dir=os.path.join(drive_dir,\"image\")\n",
    "if os.path.exists(images_dir) == False:\n",
    "  os.mkdir(images_dir)\n",
    "\n",
    "#YOLOV8 PART START\n",
    "\n",
    "yolo_dir=os.path.join(drive_dir,\"yolov8\")\n",
    "if os.path.exists(yolo_dir) == False:\n",
    "  os.mkdir(yolo_dir)\n",
    "\n",
    "labels_dir_v8=os.path.join(yolo_dir,\"labels\")\n",
    "if os.path.exists(labels_dir_v8) == False:\n",
    "  os.mkdir(labels_dir_v8)\n",
    "\n",
    "if os.path.exists(labels_dir_v8+\"/train\") == False:\n",
    "  os.mkdir(labels_dir_v8+\"/train\")\n",
    "if os.path.exists(labels_dir_v8+\"/val\") == False:\n",
    "  os.mkdir(labels_dir_v8+\"/val2\")\n",
    "\n",
    "images_dir_v8=os.path.join(yolo_dir,\"images\")\n",
    "if os.path.exists(images_dir_v8) == False:\n",
    "  os.mkdir(images_dir_v8)\n",
    "\n",
    "if os.path.exists(images_dir_v8+\"/train\") == False:\n",
    "  os.mkdir(images_dir_v8+\"/train\")\n",
    "if os.path.exists(images_dir_v8+\"/val\") == False:\n",
    "  os.mkdir(images_dir_v8+\"/val\")\n",
    "\n",
    "prediction_dir=os.path.join(yolo_dir,\"predictions\")\n",
    "if os.path.exists(prediction_dir) == False:\n",
    "  os.mkdir(prediction_dir)\n",
    "\n",
    "#COCO PART START\n",
    "coco_dir=os.path.join(drive_dir,\"coco\")\n",
    "if os.path.exists(coco_dir) == False:\n",
    "  os.mkdir(coco_dir)\n",
    "\n",
    "coco_dir_test=os.path.join(drive_dir,\"coco_test\")\n",
    "if os.path.exists(coco_dir_test) == False:\n",
    "  os.mkdir(coco_dir_test)\n",
    "\n",
    "coco_weights=os.path.join(drive_dir,\"coco_weights\")\n",
    "if os.path.exists(coco_weights) == False:\n",
    "  os.mkdir(coco_weights)\n",
    "\n",
    "#KAGGLE DATASET PART\n",
    "imgs_list=list(sorted(os.listdir(train_imgs_dir)))\n",
    "idxs=list(range(len(imgs_list)))\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "train_idx=idxs[:int(0.8*len(idxs))]\n",
    "val_idx=idxs[int(0.8*len(idxs)):]\n",
    "\n",
    "\n",
    "imgs_list=list(sorted(os.listdir(train_imgs_dir)))\n",
    "\n",
    "#COCO DATASET PART\n",
    "\n",
    "f = open('./annotations/instances_train2017.json')\n",
    "data_coco = json.load(f)\n",
    "f.close()\n",
    "data_coco.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12251\n"
     ]
    }
   ],
   "source": [
    "#COCO TEST \n",
    "car_images=get_single_category(data_coco, 3)\n",
    "car_images = sorted(list(set(car_images)))\n",
    "# image_id=car_images[50]\n",
    "# image_id_str_=str(image_id)\n",
    "# image_id_pad=image_id_str_.zfill(12)\n",
    "# # print(get_bbox(data_coco, image_id))\n",
    "# url='http://images.cocodataset.org/train2017/'+image_id_pad+'.jpg'\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "# image\n",
    "print(len(car_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 11566.73it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 7585.05it/s]\n"
     ]
    }
   ],
   "source": [
    "download_coco_images(car_images, 1000, coco_dir)\n",
    "download_coco_images(car_images[5000:], 100, coco_dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(list(sorted(os.listdir(labels_dir)))) == 0:\n",
    "    logging.info(\"Populating v1 directories\")\n",
    "    populate_v1_dir(drive_dir, imgs_list, images_dir, labels_dir, train_imgs_dir)\n",
    "    logging.info(\"Populated v1 directories\")\n",
    "    \n",
    "if len(list(sorted(os.listdir(labels_dir_v8+\"/train\")))) == 0:\n",
    "    logging.info(\"Populating v8 directories\")\n",
    "    populate_v8_dir(drive_dir, imgs_list, images_dir_v8, labels_dir_v8, train_imgs_dir, val_idx)\n",
    "    logging.info(\"Populated v1 directories\")\n",
    "    \n",
    "yaml_path = os.path.join(yolo_dir,\"yolo.yaml\")\n",
    "if os.path.exists(yaml_path) == False:\n",
    "    generate_yolov8_yaml(yolo_dir, images_dir_v8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if hypers.LOAD_MODEL_v8:\n",
    "#     model = YOLO(hypers.path_best_weights_v8)\n",
    "# else:\n",
    "#     model=YOLO('yolov8m.pt')\n",
    "# model.train(data=yolo_dir+'/yolo.yaml',epochs=20,patience=5,batch=8,\n",
    "#                     lr0=0.0005,imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # metrics = model.val()\n",
    "# results, test_img_list = predict_yolo_v8(model, test_imgs_dir, prediction_dir)\n",
    "\n",
    "# imgs_name=['vid_5_400','vid_5_26720']\n",
    "# save_sample_pics(test_imgs_dir, imgs_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Yolov2Tiny(split_size=7, num_boxes=hypers.NUM_BOXES, num_classes=hypers.NUM_CLASSES).to(hypers.DEVICE)\n",
    "optimizer = optim.Adam( model.parameters(), lr=hypers.LEARNING_RATE, weight_decay=hypers.WEIGHT_DECAY )\n",
    "loss_fn = YoloLossMultiBoxes(S=7, B=hypers.NUM_BOXES, C=hypers.NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ciarp\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if hypers.DATA_AUGMENTATION:\n",
    "    transform = Compose([\n",
    "      transforms.RandomResizedCrop(size=(448, 448),scale=(hypers.MINIMUM_SCALE,1),ratio=(1,1), antialias=True),\n",
    "      transforms.RandomHorizontalFlip(p=0.5),\n",
    "      transforms.ToTensor(),\n",
    "      ])\n",
    "else:\n",
    "    transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor(),])\n",
    "\n",
    "annotations=create_annotations(images_dir) # LIST LIKE: 0     vid_4_1000.jpg   vid_4_1000.txt\n",
    "annotations_predict=create_annotations(test_imgs_dir) # LIST LIKE: 0     vid_4_1000.jpg   vid_4_1000.txt\n",
    "\n",
    "\n",
    "dataset=VOCDataset(annotations, labels_dir, images_dir, S=7, B=hypers.NUM_BOXES, C=hypers.NUM_CLASSES, transform=transform)\n",
    "dataset_predict=VOCDatasetPredict(annotations_predict, test_imgs_dir, S=7, B=hypers.NUM_BOXES, C=hypers.NUM_CLASSES, transform=transform)\n",
    "\n",
    "dataset_coco = COCODataset(data=data_coco, transform=transform,image_folder=coco_dir,filter_category=[2], S=7, B=2, C=1)\n",
    "dataset_coco_test = COCODataset(data=data_coco, transform=transform,image_folder=coco_dir_test,filter_category=[2], S=7, B=2, C=1)\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [300, 55])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=hypers.BATCH_SIZE,\n",
    "                            pin_memory=hypers.PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=hypers.BATCH_SIZE,\n",
    "                        pin_memory=hypers.PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "predict_loader = DataLoader(dataset=dataset_predict, batch_size=hypers.BATCH_SIZE,\n",
    "                        pin_memory=hypers.PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "\n",
    "coco_train_loader = DataLoader(dataset=dataset_coco, batch_size=hypers.BATCH_SIZE,\n",
    "                            pin_memory=hypers.PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "\n",
    "coco_test_loader = DataLoader(dataset=dataset_coco_test, batch_size=hypers.BATCH_SIZE,\n",
    "                            pin_memory=hypers.PIN_MEMORY, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOLO TRAINING\n",
    "\n",
    "map_array_train = []\n",
    "map_array_test = []\n",
    "for epoch in tqdm(range(10), position=0, leave=True):\n",
    "    \n",
    "    pred_boxes, target_boxes = get_bboxes(train_loader, model, iou_threshold=0.5, threshold=0.4,S=7, B=hypers.NUM_BOXES, C=hypers.NUM_CLASSES, device=hypers.DEVICE)\n",
    "    mean_avg_prec_train = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "    \n",
    "    pred_boxes, target_boxes = get_bboxes(test_loader, model, iou_threshold=0.5, threshold=0.4,S=7, B=hypers.NUM_BOXES, C=hypers.NUM_CLASSES, device=hypers.DEVICE)\n",
    "    mean_avg_prec_test = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "    \n",
    "    map_array_train.append(mean_avg_prec_train.item())\n",
    "    map_array_test.append(mean_avg_prec_test.item())\n",
    "    \n",
    "    print(f\"epoch: {epoch} Train mAP: {mean_avg_prec_train}\")\n",
    "    print(f\"epoch: {epoch} Test mAP: {mean_avg_prec_test}\")\n",
    "    \n",
    "    # k=0\n",
    "    # if (mean_avg_prec > 0.9) and (k==0):\n",
    "    #     k=1\n",
    "    #     checkpoint = {\n",
    "    #         \"state_dict\": model.state_dict(),\n",
    "    #         \"optimizer\": optimizer.state_dict(),\n",
    "    #     }\n",
    "    #     save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE, exit_training=True)\n",
    "    #     # time.sleep(10)\n",
    "\n",
    "    train_fn(train_loader, model, optimizer, loss_fn)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COCO TRAINING\n",
    "# map_array_train = []\n",
    "# map_array_test = []\n",
    "# for epoch in tqdm(range(10), position=0, leave=True):\n",
    "    \n",
    "#     if epoch % 2 == 0:\n",
    "        \n",
    "#         pred_boxes, target_boxes = get_bboxes(coco_train_loader, model, iou_threshold=0.5, threshold=0.4,S=7, B=hypers.NUM_BOXES, C=hypers.NUM_CLASSES, device=hypers.DEVICE)\n",
    "#         mean_avg_prec_train = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "    \n",
    "#         # pred_boxes, target_boxes = get_bboxes(coco_test_loader, model, iou_threshold=0.5, threshold=0.4,S=7, B=hypers.NUM_BOXES, C=hypers.NUM_CLASSES, device=hypers.DEVICE)\n",
    "#         # mean_avg_prec_test = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "    \n",
    "#         map_array_train.append(mean_avg_prec_train.item())\n",
    "#         # map_array_test.append(mean_avg_prec_test.item())\n",
    "    \n",
    "#         print(f\"epoch: {epoch} Train mAP: {mean_avg_prec_train}\")\n",
    "#         # print(f\"epoch: {epoch} Test mAP: {mean_avg_prec_test}\")\n",
    "        \n",
    "#         if mean_avg_prec_train >= max(map_array_train):\n",
    "#             checkpoint = {\n",
    "#                 \"state_dict\": model.state_dict(),\n",
    "#                 \"optimizer\": optimizer.state_dict(),\n",
    "#             }\n",
    "#             save_checkpoint(checkpoint, filename=coco_weights+\"/best_train_weights.pth\", exit_training=False)\n",
    "            \n",
    "#         # if mean_avg_prec_test >= max(map_array_test):\n",
    "#         #     checkpoint = {\n",
    "#         #         \"state_dict\": model.state_dict(),\n",
    "#         #         \"optimizer\": optimizer.state_dict(),\n",
    "#         #     }\n",
    "#         #     save_checkpoint(checkpoint, filename=coco_weights+\"/best_test_weights.pth\", exit_training=False)\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "#     train_fn(coco_train_loader, model, optimizer, loss_fn)\n",
    "# checkpoint = {\n",
    "#                 \"state_dict\": model.state_dict(),\n",
    "#                 \"optimizer\": optimizer.state_dict(),\n",
    "#             }\n",
    "# save_checkpoint(checkpoint, filename=coco_weights+\"/last_train_weights.pth\", exit_training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "load_checkpoint(torch.load(coco_weights+\"/best_train_weights.pth\"), model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_boxes, target_boxes = get_bboxes(coco_train_loader, model, iou_threshold=0.5, threshold=0.4,S=7, B=hypers.NUM_BOXES, C=hypers.NUM_CLASSES, device=hypers.DEVICE)\n",
    "mean_avg_prec_train = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "\n",
    "# pred_boxes, target_boxes = get_bboxes(coco_test_loader, model, iou_threshold=0.5, threshold=0.4,S=7, B=hypers.NUM_BOXES, C=hypers.NUM_CLASSES, device=hypers.DEVICE)\n",
    "# mean_avg_prec_test = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "\n",
    "# map_array_train.append(mean_avg_prec_train.item())\n",
    "# map_array_test.append(mean_avg_prec_test.item())\n",
    "\n",
    "# print(f\"epoch: {epoch} Train mAP: {mean_avg_prec_train}\")\n",
    "# print(f\"epoch: {epoch} Test mAP: {mean_avg_prec_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mAP: 0.2536362111568451\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train mAP: {mean_avg_prec_train}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_images(data_coco, coco_dir_test, end_index=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model.predict(coco_dir,conf=0.50,iou=0.2)\n",
    "plot_predicted_images(coco_dir, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model.predict(coco_dir_test,conf=0.50,iou=0.2)\n",
    "plot_predicted_images(coco_dir_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model.predict(coco_dir,conf=0.50,iou=0.2)\n",
    "plot_predicted_images(coco_dir, result)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
